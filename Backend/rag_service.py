# RAG ê¸°ëŠ¥ ë‹´ë‹¹ ëª¨ë“ˆ ìƒì„±
# RAG ê²€ìƒ‰/ DB ì €ì¥
import os
import uuid
import time
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential

load_dotenv()

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
SEARCH_ENDPOINT = os.getenv("AZURE_SEARCH_ENDPOINT")
SEARCH_KEY = os.getenv("AZURE_SEARCH_API_KEY")
INDEX_NAME = os.getenv("AZURE_SEARCH_INDEX_NAME")

embeddings = AzureOpenAIEmbeddings(
    azure_deployment="text-embedding-3-small",
    openai_api_version="AZURE_OPENAI_API_VERSION",
)

# ì¶”ê°€ : rag_chat.pyì—ì„œ ê°€ì ¸ì˜¨ GPT ëª¨ë¸ ì„¤ì • ì½”ë“œ
llm = AzureChatOpenAI(
    azure_deployment="AZURE_OPENAI_DEPLOYMENT_NAME", # ë³¸ì¸ ë°°í¬ ì´ë¦„ í™•ì¸ (gpt-4o)
    openai_api_version="AZURE_OPENAI_API_VERSION",
        # ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ì„ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •
)

# --- ë‚´ë¶€ í•¨ìˆ˜: RAG ê²€ìƒ‰ ---
def search_documents(query):
    try:
        search_client = SearchClient(SEARCH_ENDPOINT, INDEX_NAME, AzureKeyCredential(SEARCH_KEY))
        query_vector = embeddings.embed_query(query)
        results = search_client.search(
            search_text=query,
            vector_queries=[{"kind": "vector", "k": 3, "fields": "content_vector", "vector": query_vector}],
            select=["content", "source"]
        )
        found_context = ""
        for r in results:
            found_context += f"[ì¶œì²˜: {r['source']}]\n{r['content']}\n\n"
        return found_context if found_context else "ê´€ë ¨ ì •ë³´ ì—†ìŒ"
    except Exception as e:
        print(f"ê²€ìƒ‰ ì—ëŸ¬: {e}")
        return ""

# --- ë‚´ë¶€ í•¨ìˆ˜: DB ì €ì¥ ---
def save_to_vector_db(summary_text):
    print("ğŸ’¾ ìš”ì•½ë³¸ì„ DB(Azure Search)ì— ì €ì¥ ì¤‘...")
    try:
        search_client = SearchClient(SEARCH_ENDPOINT, INDEX_NAME, AzureKeyCredential(SEARCH_KEY))
        vector = embeddings.embed_query(summary_text)
        doc = {
            "id": str(uuid.uuid4()),
            "content": summary_text,
            "source": f"{datetime.now().strftime('%Y-%m-%d %H:%M')} íšŒì˜ ìš”ì•½",
            "content_vector": vector
        }
        search_client.upload_documents(documents=[doc])
        print("âœ… DB ì €ì¥ ì™„ë£Œ!")
        return True
    except Exception as e:
        print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

# ì¶”ê°€ : rag_chat.pyì—ì„œ ê°€ì ¸ì˜¨ ask_bot í•¨ìˆ˜(ê·¸ëŒ€ë¡œ ì‚¬ìš©)
def ask_bot(user_question):
    print(f"User: {user_question}")
    
    # 1) ì§€ì‹ ê²€ìƒ‰
    context = search_documents(user_question)
    print(f"--- [ê²€ìƒ‰ëœ ì§€ì‹] ---\n{context[:100]}...\n---------------------")
    
    # 2) GPTì—ê²Œ ì§ˆë¬¸ + ì§€ì‹ ì „ë‹¬
    prompt = f"""
    ë‹¹ì‹ ì€ íšŒì˜ë¡ ê¸°ë°˜ AI ë¹„ì„œì…ë‹ˆë‹¤. ì•„ë˜ [íšŒì˜ ë‚´ìš©]ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
    ë‚´ìš©ì— ì—†ëŠ” ë‚´ìš©ì€ "íšŒì˜ë¡ì— ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
    
    [íšŒì˜ ë‚´ìš©]
    {context}
    
    [ì§ˆë¬¸]
    {user_question}
    """
    
    response = llm.invoke(prompt)
    print(f"Bot: {response.content}\n")
    return response.content